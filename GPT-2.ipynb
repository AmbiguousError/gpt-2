{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pzxl1vYX-1kk"
   },
   "source": [
    "Setup:\n",
    "\n",
    "1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
    "\n",
    "2) make a copy to your google drive, click on copy to drive in panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iW0abT07ZkhZ"
   },
   "source": [
    "Note: colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLXW02eIYpcB"
   },
   "source": [
    "clone and cd into repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5484,
     "status": "ok",
     "timestamp": 1553760319175,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -780
    },
    "id": "ICYu3w9hIJkC",
    "outputId": "4ca30ca2-769d-4d1d-e021-f18bb0fdec7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-2'...\n",
      "remote: Enumerating objects: 31, done.\u001b[K\n",
      "remote: Counting objects:   3% (1/31)   \u001b[K\r",
      "remote: Counting objects:   6% (2/31)   \u001b[K\r",
      "remote: Counting objects:   9% (3/31)   \u001b[K\r",
      "remote: Counting objects:  12% (4/31)   \u001b[K\r",
      "remote: Counting objects:  16% (5/31)   \u001b[K\r",
      "remote: Counting objects:  19% (6/31)   \u001b[K\r",
      "remote: Counting objects:  22% (7/31)   \u001b[K\r",
      "remote: Counting objects:  25% (8/31)   \u001b[K\r",
      "remote: Counting objects:  29% (9/31)   \u001b[K\r",
      "remote: Counting objects:  32% (10/31)   \u001b[K\r",
      "remote: Counting objects:  35% (11/31)   \u001b[K\r",
      "remote: Counting objects:  38% (12/31)   \u001b[K\r",
      "remote: Counting objects:  41% (13/31)   \u001b[K\r",
      "remote: Counting objects:  45% (14/31)   \u001b[K\r",
      "remote: Counting objects:  48% (15/31)   \u001b[K\r",
      "remote: Counting objects:  51% (16/31)   \u001b[K\r",
      "remote: Counting objects:  54% (17/31)   \u001b[K\r",
      "remote: Counting objects:  58% (18/31)   \u001b[K\r",
      "remote: Counting objects:  61% (19/31)   \u001b[K\r",
      "remote: Counting objects:  64% (20/31)   \u001b[K\r",
      "remote: Counting objects:  67% (21/31)   \u001b[K\r",
      "remote: Counting objects:  70% (22/31)   \u001b[K\r",
      "remote: Counting objects:  74% (23/31)   \u001b[K\r",
      "remote: Counting objects:  77% (24/31)   \u001b[K\r",
      "remote: Counting objects:  80% (25/31)   \u001b[K\r",
      "remote: Counting objects:  83% (26/31)   \u001b[K\r",
      "remote: Counting objects:  87% (27/31)   \u001b[K\r",
      "remote: Counting objects:  90% (28/31)   \u001b[K\r",
      "remote: Counting objects:  93% (29/31)   \u001b[K\r",
      "remote: Counting objects:  96% (30/31)   \u001b[K\r",
      "remote: Counting objects: 100% (31/31)   \u001b[K\r",
      "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 212 (delta 16), reused 20 (delta 8), pack-reused 181\u001b[K\n",
      "Receiving objects: 100% (212/212), 4.37 MiB | 18.72 MiB/s, done.\n",
      "Resolving deltas: 100% (109/109), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nshepperd/gpt-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1553760676689,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -780
    },
    "id": "6eEIs3ApZUVO",
    "outputId": "c77d5c70-a65a-44f0-ffa5-5b27c239d190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd gpt-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qtn1qZPgZLb0"
   },
   "source": [
    "install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23826,
     "status": "ok",
     "timestamp": 1553760361098,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -780
    },
    "id": "434oOx0bZH6J",
    "outputId": "5a5d91bc-b945-45de-e569-8209c6288dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
      "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K    100% |████████████████████████████████| 604kB 8.2MB/s \n",
      "\u001b[?25hCollecting requests==2.21.0 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 23.1MB/s \n",
      "\u001b[?25hCollecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 22.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.22)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
      "Building wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mspacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fire, regex, requests, tqdm\n",
      "  Found existing installation: regex 2018.1.10\n",
      "    Uninstalling regex-2018.1.10:\n",
      "      Successfully uninstalled regex-2018.1.10\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "Successfully installed fire-0.1.3 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "requests"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvUQhgK3PQ4L"
   },
   "source": [
    "mount drive to access google drive for saving and accessing checkpoints later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28871,
     "status": "ok",
     "timestamp": 1553760401731,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -780
    },
    "id": "FNpf6R4ahYSN",
    "outputId": "0d0c1aa9-ac10-4f0d-aa13-b75ffb046a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1hrgeKFYsuE"
   },
   "source": [
    "download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13942,
     "status": "ok",
     "timestamp": 1553760750537,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -780
    },
    "id": "A498TySgHYyF",
    "outputId": "02717f27-b96f-4a08-b4a5-a5ce9716d1f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\r",
      "Fetching checkpoint: 1.00kit [00:00, 509kit/s]                                                      \n",
      "\r",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\r",
      "Fetching encoder.json: 1.04Mit [00:00, 32.1Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 496kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 49.5Mit/s]                                  \n",
      "Fetching model.ckpt.index: 6.00kit [00:00, 3.63Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 472kit [00:00, 24.1Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 33.3Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python3 download_model.py 117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oJPQtdLbbeK"
   },
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KzSbAvePgsI"
   },
   "source": [
    "fetch checkpoints if you have them saved in google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA2Wk7yIPmS6"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0p--9zwqQRTc"
   },
   "source": [
    "\n",
    "lets get our text to train on, in this case from project gutenberg, A Tale of Two Cities, by Charles Dickens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOCvrs-DHvxa"
   },
   "outputs": [],
   "source": [
    "# !wget https://www.gutenberg.org/files/98/98-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPfJ5b3CQXqr"
   },
   "source": [
    "\n",
    "start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "colab_type": "code",
    "id": "pEn_ihcGI00T",
    "outputId": "0a3945b3-7912-4b0a-97c1-157d71cda80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-28 08:33:57.040517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-03-28 08:33:57.044607: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1e1a680 executing computations on platform Host. Devices:\n",
      "2019-03-28 08:33:57.044675: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-03-28 08:33:57.244871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-03-28 08:33:57.245451: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1e19fa0 executing computations on platform CUDA. Devices:\n",
      "2019-03-28 08:33:57.245491: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-03-28 08:33:57.245896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-03-28 08:33:57.245931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-03-28 08:33:58.848775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-28 08:33:58.848831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-03-28 08:33:58.848849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-03-28 08:33:58.849107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "2019-03-28 08:34:16.302175: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
      "2019-03-28 08:34:16.776693: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
      "2019-03-28 08:34:16.837878: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
      "2019-03-28 08:34:16.922200: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
      "2019-03-28 08:34:17.222438: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
      "Loading checkpoint checkpoint/run1/model-2401\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "Loading dataset...\n",
      "100% 1/1 [00:00<00:00,  2.74it/s]\n",
      "dataset has 30042 tokens\n",
      "Training...\n",
      "2019-03-28 08:34:31.806104: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "[2402 | 12.32] loss=0.02 avg=0.02\n",
      "[2403 | 16.72] loss=0.02 avg=0.02\n",
      "[2404 | 21.09] loss=0.02 avg=0.02\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./train.py --dataset /content/gpt-2/hosking.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vS1RJJDFOPnb"
   },
   "source": [
    "save our checkpoints to start training again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RySjdyS3XPxK"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/backup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backup of checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JretqG1zOXdi"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6D-i7vERWbNS"
   },
   "source": [
    "use your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeETvWvrbKga"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmnSrXqtfRbq"
   },
   "source": [
    "Conditional samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utJj-iY4gHwE"
   },
   "outputs": [],
   "source": [
    "!python3 src/interactive_conditional_samples.py --top_k 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8rSqkGxg5OK"
   },
   "source": [
    "Unconditional samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LaQUEnRxWc3c"
   },
   "outputs": [],
   "source": [
    "!python3 src/generate_unconditional_samples.py | tee /tmp/samples"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of GPT-2.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/AmbiguousError/gpt-2-colab/blob/master/GPT_2.ipynb",
     "timestamp": 1553762088710
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
