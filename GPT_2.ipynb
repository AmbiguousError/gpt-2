{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GPT-2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmbiguousError/gpt-2/blob/master/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EoUDfc3rWngR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Constructive](https://imgs.xkcd.com/comics/constructive.png)"
      ]
    },
    {
      "metadata": {
        "id": "Pzxl1vYX-1kk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup:\n",
        "\n",
        "1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
        "\n",
        "2) make a copy to your google drive, click on copy to drive in panel"
      ]
    },
    {
      "metadata": {
        "id": "iW0abT07ZkhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."
      ]
    },
    {
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "clone and cd into repo"
      ]
    },
    {
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "outputId": "b62b0de3-68a6-49c7-b9b4-43881cea3d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 212 (delta 16), reused 20 (delta 8), pack-reused 181\u001b[K\n",
            "Receiving objects: 100% (212/212), 4.37 MiB | 3.20 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "outputId": "8ad87bd6-9829-4fc0-a6a6-e4e8ae43198f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "install requirements"
      ]
    },
    {
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "outputId": "6526f664-8d2f-4ad9-ea63-3793ee84d345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 23.2MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0 (from -r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.4MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.22)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, requests, tqdm\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WvUQhgK3PQ4L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "mount drive to access google drive for saving and accessing checkpoints later"
      ]
    },
    {
      "metadata": {
        "id": "FNpf6R4ahYSN",
        "colab_type": "code",
        "outputId": "f3c419b9-ddde-4ab5-da89-d0315766ab8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*download* the model"
      ]
    },
    {
      "metadata": {
        "id": "9oqeROfICjoN",
        "colab_type": "code",
        "outputId": "d28c25d5-69b6-45f9-8222-d8ddc7af6b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lcmz9WN5jPbe",
        "colab_type": "code",
        "outputId": "f4461d13-9307-4cfa-d478-ab6c6960b8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gpt-2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A498TySgHYyF",
        "outputId": "ee96fb2d-f49e-4264-e914-b6ef77f296f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 517kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 36.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 525kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:14, 34.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.46Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 33.7Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 34.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KzSbAvePgsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "fetch checkpoints if you have them saved in google drive"
      ]
    },
    {
      "metadata": {
        "id": "cA2Wk7yIPmS6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0p--9zwqQRTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "lets get our text to train on, in this case from project gutenberg, A Tale of Two Cities, by Charles Dickens"
      ]
    },
    {
      "metadata": {
        "id": "QOCvrs-DHvxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPfJ5b3CQXqr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "start training"
      ]
    },
    {
      "metadata": {
        "id": "pEn_ihcGI00T",
        "colab_type": "code",
        "outputId": "3960b4b2-fed4-4bfe-96d0-29e1aefb5216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8583
        }
      },
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=src ./train.py --dataset /content/gpt-2/hosking.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-28 19:07:43.281271: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-03-28 19:07:43.281748: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1885860 executing computations on platform Host. Devices:\n",
            "2019-03-28 19:07:43.281817: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-03-28 19:07:43.454684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-03-28 19:07:43.455375: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1885440 executing computations on platform CUDA. Devices:\n",
            "2019-03-28 19:07:43.455450: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-03-28 19:07:43.455958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-03-28 19:07:43.456014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-03-28 19:07:44.924112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-03-28 19:07:44.924205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-03-28 19:07:44.924218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-03-28 19:07:44.924601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2019-03-28 19:08:03.487377: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
            "2019-03-28 19:08:04.000727: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
            "2019-03-28 19:08:04.062027: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
            "2019-03-28 19:08:04.148958: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
            "2019-03-28 19:08:04.480879: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.\n",
            "Loading checkpoint checkpoint/run1/model-3415\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00,  2.70it/s]\n",
            "dataset has 30042 tokens\n",
            "Training...\n",
            "2019-03-28 19:08:21.468903: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[3416 | 11.90] loss=0.01 avg=0.01\n",
            "[3417 | 15.96] loss=0.01 avg=0.01\n",
            "[3418 | 20.04] loss=0.01 avg=0.01\n",
            "[3419 | 24.12] loss=0.02 avg=0.01\n",
            "[3420 | 28.20] loss=0.01 avg=0.01\n",
            "[3421 | 32.30] loss=0.01 avg=0.01\n",
            "[3422 | 36.39] loss=0.01 avg=0.01\n",
            "[3423 | 40.48] loss=0.01 avg=0.01\n",
            "[3424 | 44.57] loss=0.02 avg=0.01\n",
            "[3425 | 48.67] loss=0.01 avg=0.01\n",
            "[3426 | 52.77] loss=0.02 avg=0.01\n",
            "[3427 | 56.85] loss=0.02 avg=0.01\n",
            "[3428 | 60.93] loss=0.01 avg=0.01\n",
            "[3429 | 65.03] loss=0.02 avg=0.01\n",
            "[3430 | 69.12] loss=0.02 avg=0.01\n",
            "[3431 | 73.21] loss=0.01 avg=0.01\n",
            "[3432 | 77.30] loss=0.01 avg=0.01\n",
            "[3433 | 81.39] loss=0.01 avg=0.01\n",
            "[3434 | 85.51] loss=0.02 avg=0.01\n",
            "[3435 | 89.60] loss=0.02 avg=0.01\n",
            "[3436 | 93.70] loss=0.01 avg=0.01\n",
            "[3437 | 97.78] loss=0.02 avg=0.01\n",
            "[3438 | 101.87] loss=0.01 avg=0.01\n",
            "[3439 | 105.98] loss=0.01 avg=0.01\n",
            "[3440 | 110.09] loss=0.01 avg=0.01\n",
            "[3441 | 114.17] loss=0.02 avg=0.01\n",
            "[3442 | 118.27] loss=0.01 avg=0.01\n",
            "[3443 | 122.36] loss=0.01 avg=0.01\n",
            "[3444 | 126.47] loss=0.01 avg=0.01\n",
            "[3445 | 130.59] loss=0.02 avg=0.01\n",
            "[3446 | 134.70] loss=0.01 avg=0.01\n",
            "[3447 | 138.82] loss=0.01 avg=0.01\n",
            "[3448 | 142.94] loss=0.02 avg=0.01\n",
            "[3449 | 147.06] loss=0.01 avg=0.01\n",
            "[3450 | 151.16] loss=0.01 avg=0.01\n",
            "[3451 | 155.24] loss=0.02 avg=0.01\n",
            "[3452 | 159.33] loss=0.01 avg=0.01\n",
            "[3453 | 163.41] loss=0.01 avg=0.01\n",
            "[3454 | 167.51] loss=0.01 avg=0.01\n",
            "[3455 | 171.62] loss=0.01 avg=0.01\n",
            "[3456 | 175.72] loss=0.01 avg=0.01\n",
            "[3457 | 179.82] loss=0.02 avg=0.01\n",
            "[3458 | 183.94] loss=0.02 avg=0.01\n",
            "[3459 | 188.04] loss=0.02 avg=0.01\n",
            "[3460 | 192.14] loss=0.01 avg=0.01\n",
            "[3461 | 196.23] loss=0.01 avg=0.01\n",
            "[3462 | 200.31] loss=0.01 avg=0.01\n",
            "[3463 | 204.42] loss=0.01 avg=0.01\n",
            "[3464 | 208.50] loss=0.01 avg=0.01\n",
            "[3465 | 212.60] loss=0.01 avg=0.01\n",
            "[3466 | 216.71] loss=0.02 avg=0.01\n",
            "[3467 | 220.82] loss=0.02 avg=0.01\n",
            "[3468 | 224.92] loss=0.01 avg=0.01\n",
            "[3469 | 229.01] loss=0.01 avg=0.01\n",
            "[3470 | 233.09] loss=0.01 avg=0.01\n",
            "[3471 | 237.20] loss=0.01 avg=0.01\n",
            "[3472 | 241.29] loss=0.02 avg=0.01\n",
            "[3473 | 245.37] loss=0.01 avg=0.01\n",
            "[3474 | 249.46] loss=0.01 avg=0.01\n",
            "[3475 | 253.55] loss=0.01 avg=0.01\n",
            "[3476 | 257.65] loss=0.01 avg=0.01\n",
            "[3477 | 261.77] loss=0.01 avg=0.01\n",
            "[3478 | 265.86] loss=0.01 avg=0.01\n",
            "[3479 | 269.94] loss=0.02 avg=0.01\n",
            "[3480 | 274.02] loss=0.01 avg=0.01\n",
            "[3481 | 278.11] loss=0.01 avg=0.01\n",
            "[3482 | 282.21] loss=0.01 avg=0.01\n",
            "[3483 | 286.29] loss=0.02 avg=0.01\n",
            "[3484 | 290.38] loss=0.01 avg=0.01\n",
            "[3485 | 294.48] loss=0.02 avg=0.01\n",
            "[3486 | 298.60] loss=0.01 avg=0.01\n",
            "[3487 | 302.70] loss=0.01 avg=0.01\n",
            "[3488 | 306.82] loss=0.01 avg=0.01\n",
            "[3489 | 310.95] loss=0.01 avg=0.01\n",
            "[3490 | 315.06] loss=0.01 avg=0.01\n",
            "[3491 | 319.18] loss=0.01 avg=0.01\n",
            "[3492 | 323.30] loss=0.01 avg=0.01\n",
            "[3493 | 327.41] loss=0.01 avg=0.01\n",
            "[3494 | 331.53] loss=0.01 avg=0.01\n",
            "[3495 | 335.63] loss=0.01 avg=0.01\n",
            "[3496 | 339.72] loss=0.01 avg=0.01\n",
            "[3497 | 343.80] loss=0.01 avg=0.01\n",
            "[3498 | 347.90] loss=0.01 avg=0.01\n",
            "[3499 | 352.00] loss=0.02 avg=0.01\n",
            "======== SAMPLE 1 ========\n",
            " the country with her.\n",
            "\n",
            "Well actually you can, you just have to want to, and Britain has. Let them rot in Syria, where they so willingly chose to go.\n",
            "\n",
            "Oh, you'll incur the wrath of the United Nations. But what would you rather have, a wagging finger or a crazed jihadist who's vowed to annihilate you, in the prison down the road, with knowing this country sadly, a not unreasonable chance of parole for good behaviour?\n",
            "\n",
            "You know that Ardern would let him in, you know she's driven by UN-type doctrines. She'd probably put him on the job-seeker benefit and tell him to take his time looking for work.\n",
            "\n",
            "Javid has started the international response well, America has done similar things, but is facing the inevitable legal challenges.\n",
            "\n",
            "If there is a weak socialist link here it is, sadly, us. We won't have the same resolve, we won't, I don't think, even want to have the same resolve.\n",
            "\n",
            "So Mark Taylor, jihadist and traitor, you want him back? And what are you going to say when Ardern says he's welcome home?\n",
            "\n",
            "Why shouldn't Ryan Fox and Mariah Carey work in Saudi Arabia?\n",
            "\n",
            "Golfer Ryan Fox and singer Mariah Carey seemed to have incurred the wrath of many and suffered perhaps a slightly uncomfortable weekend.\n",
            "\n",
            "Not a couple of names normally uttered in the same sentence, but both have associations with Saudi Arabia.\n",
            "\n",
            "He's in a tournament as we speak, and she's been holding concerts there. The upset of course is that, according to some, Saudi Arabia is a social or moral or political pariah and what we need is mass boycott.\n",
            "\n",
            "What we need is the Ryan Foxes and Mariah Careys shunning them and their money. For the argument goes, they are being used by the kingdom to promote normality, international participation and a certain amount of branding. Ryan Fox of course has not been singled out per se - the argument applies to everyone in the field.\n",
            "\n",
            "Of course the upset and angsting is futile, not to mention at least in part hypocritical.\n",
            "\n",
            "Do we all agree with what Saudi Arabia does and how they conduct themselves in certain spheres and aspects of life? No we do not.\n",
            "\n",
            "But if that is the criteria for boycott and isolation then we would be doing very little and never leaving home. The globe, rightly or wrongly, is festooned with activity that many don't condone. Golfers, singers, and you and I, are making not one jot of difference to any of it.\n",
            "\n",
            "A lot of it - not all - is cultural, meaning just because we don't like it, doesn't make it wrong. We also seem to forget that in isolating out the famous people for attention and some sort of action, we place upon them a role most did not ask for - that of activist.\n",
            "\n",
            "Ryan Fox said he has a job to do, and he does. Fox isn't a civil rights campaigner, or a political operative. He's a golfer. And he goes where the work is.\n",
            "\n",
            "It would seem remarkable, would it not, that we expect Fox or Carey to make a point to the Saudi Arabians when the president of the United States doesn't. Most countries don't, and most counties don't because of the cold, hard simple truth about the way the world operates.\n",
            "\n",
            "Saudi Arabia has money and oil and influence - and the more of those things you have, the more you can get away with it. Oh nations make noises - but in full knowledge that the noises mean nothing they just look good.\n",
            "\n",
            "They're seen to be doing the right thing, even if in reality it's nothing tangible. And if we are to wander down the track of righteous indignation at those who we don't agree with then travel to China is off, holidays in Hong Kong are gone, trips to South Africa are a no-go - how long do you want that list to be?\n",
            "\n",
            "If it's good for Fox and Carey surely it must be good for us? How could you sit in Denarau all those years while the Fijian people were oppressed?\n",
            "\n",
            "It's easy to ask of others what we wouldn't - and don't - of ourselves, isn't it.\n",
            "\n",
            "Don't panic - the All Blacks are still the best team in the world\n",
            "\n",
            "No one likes to see the All Blacks lose - but if you're going to lose, one: you want to lose in a game to remember.\n",
            "\n",
            "Two: You want to lose to a side that most would argue were the better team.\n",
            "\n",
            "Three: You want to lose to a side that were seen as genuine respectable contenders going in.\n",
            "\n",
            "Four: You want to lose to a side that represents a region that is closing what has been regarded as a chasm, in terms of talent.\n",
            "\n",
            "Five\n",
            "\n",
            "[3500 | 374.92] loss=0.01 avg=0.01\n",
            "[3501 | 379.01] loss=0.01 avg=0.01\n",
            "[3502 | 383.11] loss=0.01 avg=0.01\n",
            "[3503 | 387.21] loss=0.01 avg=0.01\n",
            "[3504 | 391.30] loss=0.01 avg=0.01\n",
            "[3505 | 395.38] loss=0.02 avg=0.01\n",
            "[3506 | 399.48] loss=0.02 avg=0.01\n",
            "[3507 | 403.59] loss=0.02 avg=0.01\n",
            "[3508 | 407.69] loss=0.01 avg=0.01\n",
            "[3509 | 411.78] loss=0.01 avg=0.01\n",
            "[3510 | 415.85] loss=0.01 avg=0.01\n",
            "[3511 | 419.95] loss=0.01 avg=0.01\n",
            "[3512 | 424.07] loss=0.01 avg=0.01\n",
            "[3513 | 428.19] loss=0.01 avg=0.01\n",
            "[3514 | 432.30] loss=0.01 avg=0.01\n",
            "[3515 | 436.40] loss=0.02 avg=0.01\n",
            "[3516 | 440.51] loss=0.02 avg=0.01\n",
            "[3517 | 444.59] loss=0.02 avg=0.01\n",
            "[3518 | 448.66] loss=0.02 avg=0.01\n",
            "[3519 | 452.76] loss=0.01 avg=0.01\n",
            "[3520 | 456.81] loss=0.01 avg=0.01\n",
            "[3521 | 460.90] loss=0.01 avg=0.01\n",
            "[3522 | 465.02] loss=0.01 avg=0.01\n",
            "[3523 | 469.13] loss=0.02 avg=0.01\n",
            "[3524 | 473.24] loss=0.02 avg=0.01\n",
            "[3525 | 477.35] loss=0.01 avg=0.01\n",
            "[3526 | 481.46] loss=0.01 avg=0.01\n",
            "[3527 | 485.56] loss=0.01 avg=0.01\n",
            "[3528 | 489.65] loss=0.02 avg=0.01\n",
            "[3529 | 493.74] loss=0.02 avg=0.01\n",
            "[3530 | 497.83] loss=0.02 avg=0.01\n",
            "[3531 | 501.92] loss=0.01 avg=0.01\n",
            "[3532 | 506.03] loss=0.02 avg=0.01\n",
            "[3533 | 510.13] loss=0.02 avg=0.01\n",
            "[3534 | 514.22] loss=0.01 avg=0.01\n",
            "[3535 | 518.33] loss=0.02 avg=0.01\n",
            "[3536 | 522.44] loss=0.02 avg=0.01\n",
            "[3537 | 526.56] loss=0.01 avg=0.01\n",
            "[3538 | 530.66] loss=0.02 avg=0.01\n",
            "[3539 | 534.78] loss=0.01 avg=0.01\n",
            "[3540 | 538.89] loss=0.01 avg=0.01\n",
            "[3541 | 543.00] loss=0.01 avg=0.01\n",
            "[3542 | 547.11] loss=0.01 avg=0.01\n",
            "[3543 | 551.22] loss=0.01 avg=0.01\n",
            "[3544 | 555.30] loss=0.01 avg=0.01\n",
            "[3545 | 559.38] loss=0.02 avg=0.01\n",
            "[3546 | 563.48] loss=0.01 avg=0.01\n",
            "[3547 | 567.58] loss=0.02 avg=0.01\n",
            "[3548 | 571.69] loss=0.01 avg=0.01\n",
            "[3549 | 575.79] loss=0.02 avg=0.01\n",
            "[3550 | 579.85] loss=0.01 avg=0.01\n",
            "[3551 | 583.94] loss=0.01 avg=0.01\n",
            "[3552 | 588.01] loss=0.01 avg=0.01\n",
            "[3553 | 592.12] loss=0.01 avg=0.01\n",
            "[3554 | 596.22] loss=0.02 avg=0.01\n",
            "[3555 | 600.31] loss=0.01 avg=0.01\n",
            "[3556 | 604.40] loss=0.02 avg=0.01\n",
            "[3557 | 608.48] loss=0.02 avg=0.01\n",
            "[3558 | 612.58] loss=0.01 avg=0.01\n",
            "[3559 | 616.68] loss=0.01 avg=0.01\n",
            "[3560 | 620.78] loss=0.02 avg=0.01\n",
            "[3561 | 624.89] loss=0.01 avg=0.01\n",
            "[3562 | 628.99] loss=0.01 avg=0.01\n",
            "[3563 | 633.09] loss=0.01 avg=0.01\n",
            "[3564 | 637.18] loss=0.01 avg=0.01\n",
            "[3565 | 641.29] loss=0.02 avg=0.01\n",
            "[3566 | 645.39] loss=0.01 avg=0.01\n",
            "[3567 | 649.49] loss=0.02 avg=0.01\n",
            "[3568 | 653.59] loss=0.01 avg=0.01\n",
            "[3569 | 657.67] loss=0.01 avg=0.01\n",
            "[3570 | 661.75] loss=0.01 avg=0.01\n",
            "[3571 | 665.86] loss=0.01 avg=0.01\n",
            "[3572 | 669.96] loss=0.01 avg=0.01\n",
            "[3573 | 674.07] loss=0.01 avg=0.01\n",
            "[3574 | 678.16] loss=0.02 avg=0.01\n",
            "[3575 | 682.24] loss=0.02 avg=0.01\n",
            "[3576 | 686.33] loss=0.01 avg=0.01\n",
            "[3577 | 690.44] loss=0.01 avg=0.01\n",
            "[3578 | 694.56] loss=0.02 avg=0.01\n",
            "[3579 | 698.67] loss=0.01 avg=0.01\n",
            "[3580 | 702.78] loss=0.01 avg=0.01\n",
            "[3581 | 706.90] loss=0.01 avg=0.01\n",
            "[3582 | 711.00] loss=0.02 avg=0.01\n",
            "[3583 | 715.10] loss=0.01 avg=0.01\n",
            "[3584 | 719.21] loss=0.02 avg=0.01\n",
            "[3585 | 723.30] loss=0.01 avg=0.01\n",
            "[3586 | 727.40] loss=0.02 avg=0.01\n",
            "[3587 | 731.50] loss=0.01 avg=0.01\n",
            "[3588 | 735.60] loss=0.01 avg=0.01\n",
            "[3589 | 739.70] loss=0.01 avg=0.01\n",
            "[3590 | 743.77] loss=0.01 avg=0.01\n",
            "[3591 | 747.87] loss=0.02 avg=0.01\n",
            "[3592 | 751.98] loss=0.02 avg=0.01\n",
            "[3593 | 756.09] loss=0.01 avg=0.01\n",
            "[3594 | 760.19] loss=0.01 avg=0.01\n",
            "[3595 | 764.29] loss=0.02 avg=0.01\n",
            "[3596 | 768.41] loss=0.02 avg=0.01\n",
            "[3597 | 772.52] loss=0.02 avg=0.01\n",
            "[3598 | 776.62] loss=0.01 avg=0.01\n",
            "[3599 | 780.70] loss=0.01 avg=0.01\n",
            "======== SAMPLE 1 ========\n",
            " the \"we are one\" campaign has not worked.\n",
            "\n",
            "In other words the only people allowed in were Māori or Pasifika.\n",
            "\n",
            "And what of all the cases for Māori and Pasifika that are genuine? That don't involve drug dealing, lying, and jail time? Having set the bar so low, do they all get to stay? Of course they don't.\n",
            "\n",
            "This as it turns out, as we said all along, was not complex. It was open and shut, except given one of the suspects was a bit shifty and weird.\n",
            "\n",
            "And also the year, and the country, has been plagued by scandals.\n",
            "\n",
            "In fact, it didn't have that great a year last year. Last year we got the news that you can flip your property and keep most of the profits - that was despite the Government originally saying you couldn't.\n",
            "\n",
            "We also got the news that KiwiBuild ballots have been extended due to lack of demand in some places. That, of course, was based on the fact that KiwiBuild isn't a home programme for those locked out of the market as the Government told us it was: it's for six-figure executives who actually have a lot of choice in the market anyway.\n",
            "\n",
            "Then we find out this week that the $2 billion set aside to build the 100,000 homes isn't within a mile of being enough. The Ministry of Business, Innovation and Employment says it's out by about $18b.\n",
            "\n",
            "The money set aside might build 1000 homes, not 100,000. So far you've got limited demand, prices too high and not enough start-up capital, which is why it's become a buy off the plan scheme from developers - the Government doesn't have the dough.\n",
            "\n",
            "And that's before you get to New Plymouth, where the Minister of this increasing calamity announced his programme for Marfell.\n",
            "\n",
            "Marfell has fallen by the wayside Phil Twyford suggested, and KiwiBuild would be part of its regeneration.\n",
            "\n",
            "Brilliant. The price limit for Marfell would be $450,000, certainly a lot better than places like Queenstown and Auckland. But unfortunately not that attractive as it turns out given the median price for Marfell is $326,000 - so it's over $100,000 cheaper on the open market.\n",
            "\n",
            "Well it's $326,000 if you take CoreLogic numbers. If you take council numbers it's actually $271,000 - a full $179,000 cheaper.\n",
            "\n",
            "So not only aren't KiwiBuild homes for those locked out, they're not even competitive on the open market.\n",
            "\n",
            "In fact, having checked as of yesterday on Trade Me, for $450k or under there are 27 listings all over New Plymouth currently for sale. Most of them very nice, all sorts of areas, houses, apartments and units.\n",
            "\n",
            "So what is Phil actually selling? It appears nothing more than a new home. Which is not to be dismissed, as we have said before, we seem to be short of houses. Adding to the supply can't hurt.\n",
            "\n",
            "But that's not what KiwiBuild was supposed to be about. Is the Government entering the home building market to build some houses that would have already been built really doing anything at all?\n",
            "\n",
            "And is continuing to insist that this is some level of social assistance when it blatantly isn't, politically wise? When not a week goes by now, where facts defy the spin.\n",
            "\n",
            "So demand is limited, money is short, the prices are too high, you keep the profits so they'll be flipped, two-thirds of potential customers are locked out, and that's before we get to original porky of promising 100,000 houses in 10 years (which is never going to happen).\n",
            "\n",
            "The only real house here, is the house of cards called KiwiBuild.\n",
            "\n",
            "Government's clueless about the danger Kiwi Jihadist Mark Taylor poses\n",
            "\n",
            "I can't quite work out the Winston Peters approach to Kiwi Jihadist Mark Taylor. He couldn't give a rats about him and neither, says Peters, could 99.999 per cent of Kiwis.\n",
            "\n",
            "The media might be interested, but he isn't, he says. On face value he is, of course, wrong. We are worried about Taylor, he is a recognised and documented danger to this country and its safety. Even more so, if and when he lands back here.\n",
            "\n",
            "Does Peters mean as he sits in Syria in a jail we don't care, and he can rot? If that's it, then yes, Peters is onto it.\n",
            "\n",
            "But is the Peters line more Machiavellian? Is Peters running the bad cop, to Ardern's good cop? Ardern, beholden and mesmerised by the power of the United Nations, would never in a million years consider putting on some big people pants and taking a\n",
            "\n",
            "[3600 | 802.24] loss=0.02 avg=0.01\n",
            "[3601 | 806.34] loss=0.01 avg=0.01\n",
            "[3602 | 810.46] loss=0.01 avg=0.01\n",
            "[3603 | 814.55] loss=0.01 avg=0.01\n",
            "[3604 | 818.63] loss=0.01 avg=0.01\n",
            "[3605 | 822.72] loss=0.01 avg=0.01\n",
            "[3606 | 826.82] loss=0.01 avg=0.01\n",
            "[3607 | 830.90] loss=0.01 avg=0.01\n",
            "[3608 | 834.98] loss=0.02 avg=0.01\n",
            "[3609 | 839.08] loss=0.02 avg=0.01\n",
            "[3610 | 843.21] loss=0.01 avg=0.01\n",
            "[3611 | 847.31] loss=0.01 avg=0.01\n",
            "[3612 | 851.41] loss=0.01 avg=0.01\n",
            "[3613 | 855.51] loss=0.01 avg=0.01\n",
            "[3614 | 859.59] loss=0.01 avg=0.01\n",
            "[3615 | 863.68] loss=0.01 avg=0.01\n",
            "[3616 | 867.78] loss=0.02 avg=0.01\n",
            "[3617 | 871.88] loss=0.01 avg=0.01\n",
            "[3618 | 876.00] loss=0.02 avg=0.01\n",
            "[3619 | 880.11] loss=0.02 avg=0.01\n",
            "[3620 | 884.22] loss=0.02 avg=0.01\n",
            "[3621 | 888.31] loss=0.01 avg=0.01\n",
            "[3622 | 892.38] loss=0.01 avg=0.01\n",
            "[3623 | 896.45] loss=0.01 avg=0.01\n",
            "[3624 | 900.52] loss=0.02 avg=0.01\n",
            "[3625 | 904.62] loss=0.01 avg=0.01\n",
            "[3626 | 908.71] loss=0.01 avg=0.01\n",
            "[3627 | 912.77] loss=0.02 avg=0.01\n",
            "[3628 | 916.87] loss=0.01 avg=0.01\n",
            "[3629 | 920.98] loss=0.01 avg=0.01\n",
            "[3630 | 925.09] loss=0.01 avg=0.01\n",
            "[3631 | 929.19] loss=0.02 avg=0.01\n",
            "[3632 | 933.28] loss=0.01 avg=0.01\n",
            "[3633 | 937.36] loss=0.01 avg=0.01\n",
            "[3634 | 941.46] loss=0.01 avg=0.01\n",
            "[3635 | 945.55] loss=0.02 avg=0.01\n",
            "[3636 | 949.63] loss=0.01 avg=0.01\n",
            "[3637 | 953.72] loss=0.01 avg=0.01\n",
            "[3638 | 957.84] loss=0.01 avg=0.01\n",
            "[3639 | 961.96] loss=0.02 avg=0.01\n",
            "[3640 | 966.07] loss=0.01 avg=0.01\n",
            "[3641 | 970.17] loss=0.01 avg=0.01\n",
            "[3642 | 974.27] loss=0.02 avg=0.01\n",
            "[3643 | 978.35] loss=0.01 avg=0.01\n",
            "[3644 | 982.44] loss=0.01 avg=0.01\n",
            "[3645 | 986.54] loss=0.01 avg=0.01\n",
            "[3646 | 990.66] loss=0.02 avg=0.01\n",
            "[3647 | 994.78] loss=0.01 avg=0.01\n",
            "[3648 | 998.90] loss=0.02 avg=0.01\n",
            "[3649 | 1003.01] loss=0.01 avg=0.01\n",
            "[3650 | 1007.12] loss=0.02 avg=0.01\n",
            "[3651 | 1011.24] loss=0.01 avg=0.01\n",
            "[3652 | 1015.35] loss=0.01 avg=0.01\n",
            "[3653 | 1019.46] loss=0.01 avg=0.01\n",
            "[3654 | 1023.56] loss=0.01 avg=0.01\n",
            "[3655 | 1027.65] loss=0.01 avg=0.01\n",
            "[3656 | 1031.72] loss=0.02 avg=0.01\n",
            "[3657 | 1035.82] loss=0.01 avg=0.01\n",
            "[3658 | 1039.93] loss=0.02 avg=0.01\n",
            "[3659 | 1044.06] loss=0.01 avg=0.01\n",
            "[3660 | 1048.16] loss=0.01 avg=0.01\n",
            "[3661 | 1052.27] loss=0.01 avg=0.01\n",
            "[3662 | 1056.40] loss=0.02 avg=0.01\n",
            "[3663 | 1060.50] loss=0.01 avg=0.01\n",
            "[3664 | 1064.62] loss=0.01 avg=0.01\n",
            "[3665 | 1068.69] loss=0.02 avg=0.01\n",
            "[3666 | 1072.76] loss=0.02 avg=0.01\n",
            "[3667 | 1076.86] loss=0.02 avg=0.01\n",
            "[3668 | 1080.95] loss=0.01 avg=0.01\n",
            "[3669 | 1085.02] loss=0.01 avg=0.01\n",
            "[3670 | 1089.13] loss=0.01 avg=0.01\n",
            "[3671 | 1093.24] loss=0.01 avg=0.01\n",
            "[3672 | 1097.35] loss=0.02 avg=0.01\n",
            "[3673 | 1101.45] loss=0.01 avg=0.01\n",
            "[3674 | 1105.53] loss=0.01 avg=0.01\n",
            "[3675 | 1109.61] loss=0.02 avg=0.01\n",
            "[3676 | 1113.71] loss=0.01 avg=0.01\n",
            "[3677 | 1117.81] loss=0.01 avg=0.01\n",
            "[3678 | 1121.91] loss=0.02 avg=0.01\n",
            "[3679 | 1126.00] loss=0.02 avg=0.01\n",
            "[3680 | 1130.08] loss=0.01 avg=0.01\n",
            "[3681 | 1134.18] loss=0.01 avg=0.01\n",
            "[3682 | 1138.28] loss=0.02 avg=0.01\n",
            "[3683 | 1142.37] loss=0.01 avg=0.01\n",
            "[3684 | 1146.47] loss=0.02 avg=0.01\n",
            "[3685 | 1150.56] loss=0.01 avg=0.01\n",
            "[3686 | 1154.66] loss=0.01 avg=0.01\n",
            "[3687 | 1158.77] loss=0.02 avg=0.01\n",
            "[3688 | 1162.88] loss=0.01 avg=0.01\n",
            "[3689 | 1166.98] loss=0.02 avg=0.01\n",
            "[3690 | 1171.09] loss=0.02 avg=0.01\n",
            "[3691 | 1175.17] loss=0.01 avg=0.01\n",
            "[3692 | 1179.28] loss=0.01 avg=0.01\n",
            "[3693 | 1183.37] loss=0.01 avg=0.01\n",
            "[3694 | 1187.47] loss=0.01 avg=0.01\n",
            "[3695 | 1191.57] loss=0.01 avg=0.01\n",
            "[3696 | 1195.66] loss=0.01 avg=0.01\n",
            "[3697 | 1199.73] loss=0.01 avg=0.01\n",
            "[3698 | 1203.79] loss=0.01 avg=0.01\n",
            "[3699 | 1207.84] loss=0.01 avg=0.01\n",
            "======== SAMPLE 1 ========\n",
            " what you can afford becomes irrelevant, as the decision is taken out of your hands.\n",
            "\n",
            "If there are too many of these agreements, and the more you have, the more you can get away with it.\n",
            "\n",
            "And the result of those who held together by common law.\n",
            "\n",
            "But if you were being really level-handed you could say let's suck it and see. Let's see if it's as bad as we think, given it's not yet been implemented.\n",
            "\n",
            "But the bit to watch, that few if any seem to have picked up on, is at what rate are they pinging you on CGT. The suggestion seems to be the marginal rate which means 28 or 33 per cent. That is a spectacularly high rate, Labour before they dumped it in campaigning on it last time, had it at 15 per cent ... good luck selling it at more than double that rate.\n",
            "\n",
            "But it does play to Labour's base, how many traditional Labour supporters have shares and investment homes, the sort of thing a CGT will capture.\n",
            "\n",
            "If the Labour voters see the \"rich pricks\" (Michael Cullen head of the committee making the recommendations) getting taxed and they get money in their pocket then the wider fall out may be able to be managed.\n",
            "\n",
            "And then three, growth.\n",
            "\n",
            "Most people have missed this, given the number came out just before Christmas.\n",
            "\n",
            "Our Q3 GDP was 0.3 per cent, which is abysmal.\n",
            "\n",
            "Add that to Q1 at 0.5 and Q2 at 1 per cent and you have the calendar year ending September at 1.8 per cent growth.\n",
            "\n",
            "You would need Q4's number to be 1.7 per cent to get us to the international average, to even get us within range of our major trading partners.\n",
            "\n",
            "Will we get 1.7 per cent in Q4 ? No we will not, nor indeed anywhere close to it.\n",
            "\n",
            "Which means we will in all probability be growing annually at little more than 2 % per cent which is an indictment on this government's performance.\n",
            "\n",
            "They have taken a rock star economy and strangled it.\n",
            "\n",
            "They will look to blame international uncertainty, don't believe them.\n",
            "\n",
            "The IMF forecast for global growth is an average of 3.5 per cent. We should be doing that or close, we are not.\n",
            "\n",
            "And having ended the calendar year with anaemic growth, chuck in a capital gains tax and see what that does to investment intention. And lock our workplaces back up in a way Jim Knox and Ken Douglas would have been proud of.\n",
            "\n",
            "And the three issues calamitously and spectacularly come crashing into each other.\n",
            "\n",
            "On the upside, that may make Kiwibuild look like a clever idea that's a raging success.\n",
            "\n",
            "KiwiBuild scheme is a house of cards\n",
            "\n",
            "KiwiBuild has not had the best of weeks.\n",
            "\n",
            "In fact, it didn't have that great a week last week. Last week we got the news that you can flip your property and keep most of the profits - that was despite the Government originally saying you couldn't.\n",
            "\n",
            "We also got the news that KiwiBuild ballots have been extended due to lack of demand in some places. That, of course, was based on the fact that KiwiBuild isn't a home programme for those locked out of the market as the Government told us it was: it's for six-figure executives who actually have a lot of choice in the market anyway.\n",
            "\n",
            "Then we find out this week that the $2 billion set aside to build the 100,000 homes isn't within a mile of being enough. The Ministry of Business, Innovation and Employment says it's out by about $18b.\n",
            "\n",
            "The money set aside might build 1000 homes, not 100,000. So far you've got limited demand, prices too high and not enough start-up capital, which is why it's become a buy off the plan scheme from developers - the Government doesn't have the dough.\n",
            "\n",
            "And that's before you get to New Plymouth, where the Minister of this increasing calamity announced his programme for Marfell.\n",
            "\n",
            "Marfell has fallen by the wayside Phil Twyford suggested, and KiwiBuild would be part of its regeneration.\n",
            "\n",
            "Brilliant. The price limit for Marfell would be $450,000, certainly a lot better than places like Queenstown and Auckland. But unfortunately not that attractive as it turns out given the median price for Marfell is $326,000 - so it's over $100,000 cheaper on the open market.\n",
            "\n",
            "Well it's $326,000 if you take CoreLogic numbers. If you take council numbers it's actually $271,000 - a full $179,000 cheaper.\n",
            "\n",
            "So not only aren't KiwiBuild homes for those locked out, they're not even competitive on the open market.\n",
            "\n",
            "In fact\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vS1RJJDFOPnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "save our checkpoints to start training again later"
      ]
    },
    {
      "metadata": {
        "id": "JretqG1zOXdi",
        "colab_type": "code",
        "outputId": "b938c15b-47bf-4e58-d9ca-477b664006e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/gpt-2/checkpoint/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6D-i7vERWbNS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "use your trained model"
      ]
    },
    {
      "metadata": {
        "id": "VeETvWvrbKga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmnSrXqtfRbq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conditional samples"
      ]
    },
    {
      "metadata": {
        "id": "utJj-iY4gHwE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 src/interactive_conditional_samples.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8rSqkGxg5OK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "unconditional samples"
      ]
    },
    {
      "metadata": {
        "id": "LaQUEnRxWc3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 src/generate_unconditional_samples.py"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}